{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb8618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame, concat\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Attention, MultiHeadAttention, Flatten\n",
    "from tensorflow.keras.layers import Dropout, Dense, LSTM, Bidirectional, Attention, SimpleRNN, GRU\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import optuna\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from pylab import mpl\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import pathlib\n",
    "\n",
    "# to suppress warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# display all columns of the dataframe\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# display all rows of the dataframe\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# to display the float values upto 6 decimal places\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 7)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "# plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# 设置中文显示字体\n",
    "mpl.rcParams[\"font.sans-serif\"] = [\"Arial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac64c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_mean(df):\n",
    "    for cols in list(df.columns[df.isnull().sum() > 0]):\n",
    "        mean_val = df[cols].mean()  # 均值填充\n",
    "        df[cols].fillna(mean_val, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fillna_back(df):\n",
    "    df = df.fillna(axis=0, method=\"bfill\", limit=None)  #后一个数填充缺失值\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902690ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "     Frame a time series as a supervised learning dataset.\n",
    "     Arguments:\n",
    "     data: Sequence of observations as a list or NumPy array.\n",
    "     n_in: Number of lag observations as input (X).\n",
    "     n_out: Number of observations as output (y).\n",
    "     dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "     Returns:\n",
    "     Pandas DataFrame of series framed for supervised learning.\n",
    "     \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476462f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "df_AQI = pd.DataFrame()\n",
    "count = 0\n",
    "for dirname, _, filenames in os.walk('C:/Users/DRDE-GWALIOR/Downloads/beijing'):\n",
    "    # for dirname, _, filenames in os.walk('C:/Users/Balya/MTech/ML/PRESENTATIONS/PRSA_Data_20130301-20170228'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirname, filename)\n",
    "#         path = dirname+'/'+ filename\n",
    "        print('File path : ', path)\n",
    "        if count == 0:\n",
    "            df_AQI1 = pd.read_csv(path, encoding=\"utf-8\")\n",
    "            df_AQI = df_AQI.append(df_AQI1)\n",
    "            count = 1\n",
    "            continue\n",
    "\n",
    "        df_AQI1 = pd.read_csv(path, encoding=\"utf-8\")\n",
    "        df_AQI = df_AQI.append(df_AQI1)\n",
    "\n",
    "dataset = fillna_back(df_AQI)\n",
    "dataset = dataset.drop(['No','year','month','day','hour','wd','station'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1db3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outliers from the data set \n",
    "\n",
    "print('Before: Outliers Treating with Inter Quartile Range', dataset.shape)\n",
    "df_outlier = dataset.copy()\n",
    "# df_outlier.drop(['PM10'], axis=1, inplace=True)\n",
    "df_outlier  = df_outlier.select_dtypes(exclude ='object')\n",
    "#1st quantile\n",
    "q1 = df_outlier.quantile(0.25)\n",
    "\n",
    "#3rd quantile\n",
    "q3 = df_outlier.quantile(0.75)\n",
    "\n",
    "# IQR: The Interquartile Range is defined as the difference between the third and first quartile\n",
    "iqr = q3-q1\n",
    "# print(IQR)\n",
    "\n",
    "df_outlier = df_outlier[~((df_outlier  < (q1-(1.5*iqr))) | (df_outlier  > (q3+(1.5*iqr)))).any(axis=1)]\n",
    "\n",
    "# check the shape of the data\n",
    "print('After: Outliers Treating with Inter Quartile Range', df_outlier.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier = df_outlier.select_dtypes(exclude='object').copy()\n",
    " \n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "# ploting a boxplot to visualize the outliers in all the numeric variables\n",
    "df_outlier.boxplot()\n",
    "\n",
    "# setting plot label\n",
    "# setting text size using 'fontsize'\n",
    "plt.title('Distribution of all the Numeric Variables', fontsize = 15)\n",
    "\n",
    "# xticks() returns the x-axis ticks\n",
    "# 'rotation = vertical' rotates the x-axis labels vertically\n",
    "plt.xticks(rotation = 'vertical', fontsize = 12)\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07160de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AQI.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc20b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d89aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482624bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comp = df_comp.select_dtypes(exclude='object').copy()\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "df_out=df_outlier.copy()\n",
    "outliers_by_variables = df_out.columns\n",
    "\n",
    "plt.figure(figsize=(17,14))\n",
    "for i in range(0,outliers_by_variables.shape[0]):\n",
    "    plt.subplot(4,7, i+1)\n",
    "    plt.boxplot(df_out[outliers_by_variables[i]], autorange=True)\n",
    "    plt.title(outliers_by_variables[i], fontdict= {'fontsize': 15 ,'fontweight': 5, 'color':'000000'})\n",
    "    plt.AutoLocator.default_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00feb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_outlier.values\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b088e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train sets、validation sets and test sets\n",
    "values = reframed.values\n",
    "number=len(values)\n",
    "n_train_hours = int(number*0.7)\n",
    "n_valid_hours = int(number*0.9)\n",
    "train = values[:n_train_hours, :]\n",
    "valid = values[n_train_hours:n_valid_hours, :]\n",
    "test = values[n_valid_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-11], train[:, -11:-5]\n",
    "valid_X, valid_y = valid[:, :-11], valid[:, -11:-5]\n",
    "test_X, test_y = test[:, :-11], test[:, -11:-5]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "valid_X = valid_X.reshape((valid_X.shape[0], 1, valid_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape,\n",
    "      test_y.shape)  # [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715656c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"PM2.5\",\n",
    "    \"PM10\",\n",
    "    \"SO2\",\n",
    "    \"NO2\",\n",
    "    \"CO\",\n",
    "    \"O3\",\n",
    "     \"TEM\",\n",
    "    \"PRES\",\n",
    "    \"DEMP\",\n",
    "    \"RAIN\",\n",
    "    \"WSPM\",\n",
    "]\n",
    "\n",
    "feature_keys = [\n",
    "    \"PM2.5\",\n",
    "    \"PM10\",\n",
    "    \"SO2\",\n",
    "    \"NO2\",\n",
    "    \"CO\",\n",
    "    \"O3\",\n",
    "     \"TEM\",\n",
    "    \"PRES\",\n",
    "    \"DEMP\",\n",
    "    \"RAIN\",\n",
    "    \"WSPM\",\n",
    "\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"brown\",\n",
    "    \"cyan\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"purple\",\n",
    "    \"pink\",\n",
    "    \"gold\",\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17dabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    # Visualize training loss and validation loss\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure(dpi=300)\n",
    "    plt.plot(epochs, loss,\"green\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss,color=\"purple\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_acc(history, title):\n",
    "    # Visualize training loss and validation loss\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    epochs = range(len(acc))\n",
    "    plt.figure(dpi=300)\n",
    "    plt.plot(epochs, acc, \"b\", label=\"Training accuracy\")\n",
    "    plt.plot(epochs, val_acc, \"r\", label=\"Validation accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b46e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_visual(data1, data2):\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=3, figsize=(20, 15), dpi=300, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(feature_keys)-5):\n",
    "        key = feature_keys[i]\n",
    "        ax = data1[key].plot(\n",
    "            ax=axes[i // 3, i % 3],\n",
    "            title=\"{}\".format(titles[i]),\n",
    "            rot=25,\n",
    "            label=\"actual\"\n",
    "        )\n",
    "        ax = data2[key].plot(\n",
    "            ax=axes[i // 3, i % 3],\n",
    "            color='red',\n",
    "            rot=25,\n",
    "            label='prediction'\n",
    "        )\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # scaler.inverse_transform()\n",
    "\n",
    "\n",
    "def inv_scale(df, y):\n",
    "    max = df.max()[-11:-5]\n",
    "    min = df.min()[-11:-5]\n",
    "#     df*(max-min)+min\n",
    "    yy=y.copy()\n",
    "    for i in range(len(yy)):\n",
    "        yy[i] = yy[i]*(max-min)+min  \n",
    "    return yy\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    # move return_sequences=True\n",
    "#     inv_yhat = scaler.inverse_transform(yhat)\n",
    "    # invert scaling for actual\n",
    "#     inv_y = scaler.inverse_transform(test_y)\n",
    "    inv_yhat=inv_scale(dataset,yhat)\n",
    "    inv_y=inv_scale(dataset,test_y)\n",
    "\n",
    "    # calculate RMSE mean square error\n",
    "#     aqi_inv_y = inv_y[:, -6:]\n",
    "#     aqi_inv_yhat = inv_yhat[:, -6:]\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "    print('Test MAE: %.3f' % mae)\n",
    "    R2=r2_score(inv_y, inv_yhat)\n",
    "    print('Test R2: %.3f' % R2)\n",
    "    # Visualize true and predicted values\n",
    "    y1 = DataFrame(\n",
    "        inv_y[-100:], index=dataset.index[-100:], columns=dataset.columns[-11:-5])\n",
    "    y2 = DataFrame(\n",
    "        inv_yhat[-100:], index=dataset.index[-100:], columns=dataset.columns[-11:-5])\n",
    "    compare_visual(y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "timesteps = train_X.shape[1]\n",
    "input_dim = train_X.shape[2]\n",
    "output_dim=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a308f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input to the model, including time series data and labels\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "\n",
    "\n",
    "# Define RNN layer\n",
    "rnn_out = SimpleRNN(128,activation='relu',return_sequences = False)(inputs)\n",
    "\n",
    "# Define fully connected layer and output layer\n",
    "dense = Dense(units=128, activation='relu')(rnn_out)\n",
    "\n",
    "output = Dense(units=output_dim, activation='relu')(dense)\n",
    "\n",
    "# Define the model and compile it\n",
    "rnn = Model(inputs=inputs,  outputs=output)\n",
    "\n",
    "\n",
    "rnn.compile(loss='mse', optimizer=keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=0,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "# fit network\n",
    "history = rnn.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(valid_X, valid_y),\n",
    "                     callbacks=[es_callback, modelckpt_callback]\n",
    "                    )\n",
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "visualize_acc(history, \"Training and Validation Accuracy\")\n",
    "\n",
    "# evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2398ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "visualize_acc(history, \"Training and Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input to the model, including time series data and labels\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "\n",
    "\n",
    "# Define GRU Layer\n",
    "gru_out = GRU(128, activation='relu', return_sequences=False)(inputs)\n",
    "\n",
    "# Define fully connected layer and output layer\n",
    "dense = Dense(units=128, activation='relu')(gru_out)\n",
    "\n",
    "output = Dense(units=output_dim, activation='relu')(dense)\n",
    "\n",
    "# Define the model and compile it\n",
    "gru = Model(inputs=inputs,  outputs=output)\n",
    "\n",
    "\n",
    "gru.compile(loss='mse', optimizer=keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=0,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "# fit network\n",
    "history = gru.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(valid_X, valid_y),\n",
    "                     callbacks=[es_callback, modelckpt_callback]\n",
    "                    )\n",
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "visualize_acc(history, \"Training and Validation Accuracy\")\n",
    "\n",
    "# evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d17793",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "\n",
    "\n",
    "# 定义LSTM层和多头注意力层\n",
    "lstm_out = LSTM(128, activation='relu', return_sequences=False)(inputs)\n",
    "\n",
    "# 定义全连接层和输出层\n",
    "dense = Dense(units=128, activation='relu')(lstm_out)\n",
    "\n",
    "output = Dense(units=output_dim, activation='relu')(dense)\n",
    "\n",
    "# 定义模型并进行编译\n",
    "lstm = Model(inputs=inputs,  outputs=output)\n",
    "\n",
    "\n",
    "lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# plot_model(model, to_file=\"Bi-LSTM-MultiheadAtt.png\",\n",
    "#            dpi=300, show_shapes=True)\n",
    "\n",
    "path_checkpoint = \"model_checkpoint_MultiAttLSTM_aqiPre.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=0,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "# fit network\n",
    "history = lstm.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(valid_X, valid_y),\n",
    "#                     callbacks=[es_callback, modelckpt_callback]\n",
    "                    )\n",
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "visualize_acc(history, \"Training and Validation Accuracy\")\n",
    "\n",
    "# evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4893362",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "\n",
    "\n",
    "\n",
    "lstm_out = Bidirectional(\n",
    "    LSTM(units=128, activation='relu', return_sequences=True))(inputs)\n",
    "multihead_attn = MultiHeadAttention(\n",
    "    num_heads=4, key_dim=32)(lstm_out, lstm_out)\n",
    "\n",
    "\n",
    "flatten = Flatten()(multihead_attn)\n",
    "dense = Dense(units=128, activation='relu')(flatten)\n",
    "\n",
    "output = Dense(units=output_dim, activation='relu')(dense)\n",
    "\n",
    "\n",
    "BiLSTMultiheadAtt = Model(inputs=inputs,  outputs=output)\n",
    "\n",
    "\n",
    "BiLSTMultiheadAtt.compile(loss='mse', optimizer=keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_checkpoint = \"model_checkpoint_MultiAttLSTM_aqiPre.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=0,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "history = BiLSTMultiheadAtt.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(valid_X, valid_y),\n",
    "\n",
    "                    )\n",
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "visualize_acc(history, \"Training and Validation Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb541fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss: 0.0056 - accuracy: 0.8156 - val_loss: 0.0065 - val_accuracy: 0.8058\n",
    "#loss: 0.0058 - accuracy: 0.8138 - val_loss: 0.0066 - val_accuracy: 0.8003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "\n",
    "\n",
    "\n",
    "lstm_out = Bidirectional(\n",
    "    LSTM(units=128, activation='relu', return_sequences=True))(inputs)\n",
    "multihead_attn = MultiHeadAttention(\n",
    "    num_heads=4, key_dim=32)(lstm_out, lstm_out)\n",
    "\n",
    "\n",
    "flatten = Flatten()(multihead_attn)\n",
    "dense = Dense(units=128, activation='relu')(flatten)\n",
    "\n",
    "output = Dense(units=output_dim, activation='relu')(dense)\n",
    "\n",
    "\n",
    "BiLSTMultiheadAtt = Model(inputs=inputs,  outputs=output)\n",
    "\n",
    "\n",
    "BiLSTMultiheadAtt.compile(loss='mse', optimizer=keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "path_checkpoint = \"model_checkpoint_MultiAttLSTM_aqiPre.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=0,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "history = BiLSTMultiheadAtt.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(valid_X, valid_y),\n",
    "\n",
    "                    )\n",
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "visualize_acc(history, \"Training and Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfeb1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115deb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256ba28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
